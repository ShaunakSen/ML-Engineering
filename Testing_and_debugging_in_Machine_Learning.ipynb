{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Testing and debugging in Machine Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMmAuG6p238zXIjV5VtMFUV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/ML_Deployment/blob/master/Testing_and_debugging_in_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PN3t7OJrRKz",
        "colab_type": "text"
      },
      "source": [
        "## Testing and Debugging in Machine Learning\n",
        "\n",
        "> Based on the course by Google: https://developers.google.com/machine-learning/testing-debugging\n",
        "\n",
        "---\n",
        "\n",
        "![](https://developers.google.com/machine-learning/testing-debugging/images/machine-learning-development-process.svg)\n",
        "\n",
        "### Overview of Debugging ML Models\n",
        "\n",
        "You debug your ML model to make the model work. Once your model is working, you optimize the model's quality for production-readiness. This section covers both the debugging and optimization steps.\n",
        "\n",
        "### How is ML Debugging Different?\n",
        "\n",
        "Before diving into ML debugging, let’s understand what differentiates debugging ML models from debugging typical programs. *Unlike typical programs, poor quality in an ML model does not imply the presence of a bug*. Instead, to debug poor performance in a model, you investigate a broader range of causes than you would in traditional programming.\n",
        "\n",
        "For example, here are a few causes for poor model performance:\n",
        "\n",
        "- Features lack predictive power.\n",
        "- Hyperparameters are set to nonoptimal values.\n",
        "- Data contains errors and anomalies.\n",
        "- Feature engineering code contains bugs.\n",
        "\n",
        "Debugging ML models is complicated by the time it takes to run your experiments. Given the longer iteration cycles, and the larger error space, debugging ML models is uniquely challenging.\n",
        "\n",
        "### ML Model Development Process\n",
        "\n",
        "If you follow best practices for developing your ML model, then debugging your ML model will be simpler. These best practices are as follows:\n",
        "\n",
        "1. Start with a simple model that uses one or two features. Starting with a simple, easily debuggable model helps you narrow down the many possible causes for poor model performance.\n",
        "\n",
        "2. Get your model working by trying different features and hyperparameter values. Keep your model as simple as possible to simplify debugging.\n",
        "\n",
        "3. Optimize your model by iteratively trying these changes:\n",
        "    - adding features\n",
        "    - tuning hyperparameters\n",
        "    - increasing model capacity\n",
        "\n",
        "4. After each change to your model, revisit your metrics and check whether model quality increases. If not, then debug your model as described in this course.\n",
        "\n",
        "5. As you iterate, ensure you add complexity to your model slowly and incrementally.\n",
        "\n",
        "> **Note**: For certain domains like NLP or image recognition, you should start with a complex model.\n",
        "\n",
        "### ML Debugging: Programming Exercise\n",
        "\n",
        "Link to the notebook: https://colab.research.google.com/drive/18F-yejBmIGtOH72Qeb0zqJa4tnpqN8A6\n",
        "\n",
        "### Data and Feature Debugging\n",
        "\n",
        "Low-quality data will significantly affect your model's performance. It's much easier to detect low-quality data at input instead of guessing at its existence after your model predicts badly. Monitor your data by following the advice in this section.\n",
        "\n",
        "**Validate Input Data Using a Data Schema**\n",
        "\n",
        "To monitor your data, you should continuously check your data against expected statistical values by writing rules that the data must satisfy. This collection of rules is called a data schema. Define a data schema by following these steps:\n",
        "\n",
        "1. For your feature data, understand the range and distribution. For categorical features, understand the set of possible values.\n",
        "\n",
        "2. Encode your understanding into rules defined in the schema. Examples of rules are:\n",
        "\n",
        "- Ensure that user-submitted ratings are always between 1 and 5.\n",
        "- Check that “the” occurs most frequently (for an English text feature).\n",
        "- Check that categorical features have values from a fixed set.\n",
        "\n",
        "3. Test your data against the data schema. Your schema should catch data errors such as:\n",
        "\n",
        "- anomalies\n",
        "- unexpected values of categorical variables\n",
        "- unexpected data distributions\n",
        "\n",
        "> TODO: can be implemented\n",
        "\n",
        "**Ensure Splits are Good Quality**\n",
        "\n",
        "Your test and training splits must be equally representative of your input data. If the test and training splits are statistically different, then training data will not help predict the test data.\n",
        "\n",
        "Monitor the statistical properties of your splits. If the properties diverge, raise a flag. Further, test that the ratio of examples in each split stays constant. For example, if your data is split 80:20, that ratio should not change.\n",
        "\n",
        "> TODO: check for statistical similarity bw train and test splits\n",
        "\n",
        "**Test Engineered Data**\n",
        "\n",
        "While your raw data might be valid, your model only sees engineered feature data. Because engineered data looks very different from raw input data, you need to check engineered data separately. Based on your understanding of your engineered data, write unit tests. For example, you can write unit tests to check for the following conditions:\n",
        "\n",
        "- All numeric features are scaled, for example, between 0 and 1.\n",
        "- One-hot encoded vectors only contain a single 1 and N-1 zeroes.\n",
        "- Missing data is replaced by mean or default values.\n",
        "- Data distributions after transformation conform to expectations. For example, if you've normalized using z-scores, the mean of the z-scores is 0.\n",
        "- Outliers are handled, such as by scaling or clipping.\n",
        "\n",
        "> TODO: write such unit tests for engineered features\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMAAwusjFcqY",
        "colab_type": "text"
      },
      "source": [
        "### Model Debugging\n",
        "\n",
        "The first step in debugging your model is Data Debugging. After debugging your data, follow these steps to continue debugging your model, detailed in the following sections:\n",
        "\n",
        "**Check that the Model can Predict Labels**\n",
        "\n",
        "Before debugging your model, try to determine whether your features encode predictive signals. You can find linear correlations between individual features and labels by using correlation matrices. \n",
        "\n",
        "However, correlation matrices will not detect nonlinear correlations between features and labels. Instead, choose 10 examples from your dataset that your model can easily learn from. Alternatively, use synthetic data that is easily learnable. For instance, a classifier can easily learn linearly-separable examples while a regressor can easily learn labels that correlate highly with a feature cross (A synthetic feature formed by taking a Cartesian product of individual binary features obtained from categorical data or from continuous features via bucketing. Feature crosses help represent nonlinear relationships.). Then, ensure your model can achieve very small loss on these 10 easily-learnable examples.\n",
        "\n",
        "Using a few examples that are easily learnable simplifies debugging by reducing the opportunities for bugs. You can further simplify your model by switching to the simpler gradient descent algorithm instead of a more advanced optimization algorithm.\n",
        "\n",
        "**Establish a Baseline**\n",
        "\n",
        "Comparing your model against a baseline is a quick test of the model's quality. When developing a new model, define a baseline by using a simple heuristic to predict the label. If your trained model performs worse than its baseline, you need to improve your model.\n",
        "\n",
        "Examples of baselines are:\n",
        "\n",
        "- Using a linear model trained solely on the most predictive feature.\n",
        "- In classification, always predicting the most common label.\n",
        "- In regression, always predicting the mean value.\n",
        "\n",
        "Once you validate a version of your model in production, you can use that model version as a baseline for newer model versions. Therefore, you can have multiple baselines of different complexities. Testing against baselines helps justify adding complexity to your model. *A more complex model should always perform better than a less complex model or baseline*.\n",
        "\n",
        "**Implement Tests for ML Code**\n",
        "\n",
        "The testing process to catch bugs in ML code is similar to the testing process in traditional debugging. You'll write unit tests to detect bugs. Examples of code bugs in ML are:\n",
        "\n",
        "- Hidden layers that are configured incorrectly.\n",
        "- Data normalization code that returns NaNs.\n",
        "\n",
        "\n",
        "A sanity check for the presence of code bugs is to include your label in your features and train your model. If your model does not work, then it definitely has a bug.\n",
        "\n",
        "> TODO: test cases on various hyperparameters of the models\n",
        "for e.g col_by_sample should be > 0.5 always\n",
        "\n",
        "**Adjust Hyperparameter Values**\n",
        "\n",
        "![](https://i.ibb.co/ZLk0JW6/diag1.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut2IP_A_qH-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}