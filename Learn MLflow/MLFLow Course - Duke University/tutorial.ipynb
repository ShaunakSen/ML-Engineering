{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLOps Tools: MLflow and Hugging Face\n",
    "\n",
    "> https://www.coursera.org/learn/mlops-mlflow-huggingface-duke/home/week/1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import choice\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow import log_metric, log_param, log_artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DEBUG'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_param(\"threshold\", 3)\n",
    "log_param(\"verbosity\", \"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_metric(\"timestamp\", 3000)\n",
    "log_metric(\"TTC\", 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_artifact('./data/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next task we would like to track some random metrics\n",
    "\n",
    "Let us first  create an experiment:\n",
    "\n",
    "```\n",
    "> mlflow experiments create --experiment-name log-metrics-demo\n",
    "> Created experiment 'log-metrics-demo' with id 384253852139107280\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/shaunaksen/Documents/personal-projects/ML_Deployment/Learn%20MLflow/MLFLow%20Course%20-%20Duke%20University/mlruns/384253852139107280', creation_time=1690609937121, experiment_id='384253852139107280', last_update_time=1690609937121, lifecycle_stage='active', name='log-metrics-demo', tags={}>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"log-metrics-demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk 87\n",
      "disk 78\n",
      "disk 34\n",
      "ram 77\n",
      "disk 82\n",
      "disk 19\n",
      "ram 51\n",
      "cpu 47\n",
      "disk 47\n",
      "cpu 47\n"
     ]
    }
   ],
   "source": [
    "# lets log some metrics\n",
    "metric_names = [\"cpu\", \"ram\", \"disk\"]\n",
    "percentages = [i for i in range(0, 101)]\n",
    "\n",
    "for i in range(10):\n",
    "    print (choice(metric_names), choice(percentages))\n",
    "    log_metric(key=choice(metric_names), value=choice(percentages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically what happened here is\n",
    "- we created a new experiment\n",
    "- we set this experiment in mlflow\n",
    "- we logged a bunch of metrics under this experiment\n",
    "\n",
    "So if you see in /mlruns, there should be a folder corresponding to the id of the experiment we just created\n",
    "\n",
    "![](https://imgur.com/En296nb.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if an active run exists, end it\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/shaunaksen/Documents/personal-projects/ML_Deployment/Learn%20MLflow/MLFLow%20Course%20-%20Duke%20University/mlruns/384253852139107280', creation_time=1690609937121, experiment_id='384253852139107280', last_update_time=1690609937121, lifecycle_stage='active', name='log-metrics-demo', tags={}>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"log-metrics-demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets log some metrics\n",
    "metric_names = [\"cpu\"]\n",
    "percentages = [i for i in range(0, 101)]\n",
    "\n",
    "with mlflow.start_run(run_name=\"cpu_new\"):\n",
    "\n",
    "    for i in range(400):\n",
    "        # print (choice(metric_names), choice(percentages))\n",
    "        log_metric(key=\"cpu_new\", value=choice(percentages))\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also query the metrics using the mlflow UI search box:\n",
    "\n",
    "![](https://imgur.com/GszAvYj.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters, Version, Artifacts and Metrics\n",
    "\n",
    "> https://github.com/databricks/mlflow-example-sklearn-elasticnet-wine/blob/master/train.py\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    return rmse, mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the wine-quality csv file (make sure you're running this from the root of MLflow!)\n",
    "wine_path = \"/Users/shaunaksen/Documents/personal-projects/ML_Deployment/Learn MLflow/MLFLow Course - Duke University/data/wine-quality.csv\"\n",
    "data = pd.read_csv(wine_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3673, 12) (1225, 12)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets. (0.75, 0.25) split.\n",
    "train, test = train_test_split(data)\n",
    "\n",
    "print (train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predicted column is \"quality\" which is a scalar from [3, 9]\n",
    "train_x = train.drop([\"quality\"], axis=1)\n",
    "test_x = test.drop([\"quality\"], axis=1)\n",
    "train_y = train[[\"quality\"]]\n",
    "test_y = test[[\"quality\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, l1_ratio = 0.1, 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.create_experiment(name='wine-quality-experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/shaunaksen/Documents/personal-projects/ML_Deployment/Learn%20MLflow/MLFLow%20Course%20-%20Duke%20University/mlruns/562967201650279680', creation_time=1690620239059, experiment_id='562967201650279680', last_update_time=1690620239059, lifecycle_stage='active', name='wine-quality-experiment', tags={}>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"wine-quality-experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elasticnet model (alpha=0.100000, l1_ratio=0.200000):\n",
      "  RMSE: 0.7818770443445561\n",
      "  MAE: 0.6133216811213634\n",
      "  R2: 0.21041880316513917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shaunaksen/miniconda3/envs/shaunak_llm/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n",
    "    lr.fit(train_x, train_y)\n",
    "\n",
    "    predicted_qualities = lr.predict(test_x)\n",
    "\n",
    "    (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n",
    "\n",
    "    print(\"Elasticnet model (alpha=%f, l1_ratio=%f):\" % (alpha, l1_ratio))\n",
    "    print(\"  RMSE: %s\" % rmse)\n",
    "    print(\"  MAE: %s\" % mae)\n",
    "    print(\"  R2: %s\" % r2)\n",
    "\n",
    "    mlflow.log_param(\"alpha\", alpha)\n",
    "    mlflow.log_param(\"l1_ratio\", l1_ratio)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "\n",
    "    mlflow.sklearn.log_model(lr, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters__:\n",
    "\n",
    "Parameters represent the input settings or configurations of your machine learning model or script. They are the variables that you set before running your code and remain constant throughout the run. For example, parameters can include hyperparameters like learning rate, number of hidden units in a neural network, regularization strength, etc.\n",
    "\n",
    "__Metrics__:\n",
    "\n",
    "Metrics are the numerical values that you want to track to evaluate the performance of your model during training and testing. These can include loss, accuracy, precision, recall, F1 score, etc. Metrics are not fixed before running the code; they are computed during the run and may change for different runs, especially for different model configurations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we registered the model on mlflow, lets load that in and run inference on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pyfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the Model Registry (change <MODEL_NAME> with the actual name)\n",
    "model = mlflow.pyfunc.load_model(\"models:/elasticnet/none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlflow.pyfunc.loaded_model:\n",
       "  artifact_path: model\n",
       "  flavor: mlflow.sklearn\n",
       "  run_id: 01687e666c1d4f1a87ad43998b83f668"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlflow.pyfunc.PyFuncModel"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MLflow, `mlflow.pyfunc.PyFuncModel` is a type of model that represents a generic Python function for model inference. It allows you to package and serve models in a way that is agnostic to the underlying machine learning library or framework. This means you can log and load models trained with various machine learning libraries (e.g., scikit-learn, TensorFlow, PyTorch) as `PyFuncModel` and use a consistent API for inference.\n",
    "\n",
    "Here are some key points about `mlflow.pyfunc.PyFuncModel` and how it differs from a scikit-learn model:\n",
    "\n",
    "1. **Model Agnostic**: `PyFuncModel` is model-agnostic, meaning it can encapsulate models trained with different libraries. It enables you to log and load models without being tightly coupled to a specific machine learning framework.\n",
    "\n",
    "2. **Serialization**: When you log a model with MLflow, it serializes the model and its artifacts to a format that can be easily loaded and used for inference later. For scikit-learn models, MLflow serializes the model using pickle. For other libraries like TensorFlow or PyTorch, MLflow uses appropriate serialization methods (e.g., TensorFlow's SavedModel format, PyTorch's .pt file).\n",
    "\n",
    "3. **Inference**: To make predictions with a `PyFuncModel`, you can use the `predict()` method. The input data should match the format expected by the underlying model. In the example provided in the previous answer, we assumed a Pandas DataFrame as the input, but the format may differ depending on the model's requirements.\n",
    "\n",
    "4. **Flexibility**: Using `PyFuncModel` allows you to easily switch between different machine learning libraries without changing your inference code. You can load and use TensorFlow, PyTorch, or scikit-learn models in the same way, provided they are saved as `PyFuncModel`.\n",
    "\n",
    "5. **Deployment**: Since `PyFuncModel` is a generic Python function, you can easily deploy it for inference using different deployment solutions, like REST APIs, web services, serverless functions, or containerized environments.\n",
    "\n",
    "In contrast, a scikit-learn model (`sklearn.base.BaseEstimator`) is specific to the scikit-learn library. While it is a widely-used and powerful library for machine learning, it means that you're limited to using only scikit-learn models if you use the `BaseEstimator` type.\n",
    "\n",
    "With `mlflow.pyfunc.PyFuncModel`, MLflow provides a more flexible and interoperable way of handling models, which can be advantageous in projects that involve different machine learning libraries or when you want to decouple your inference code from the model's training framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_qualities_loaded_model = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7196.524270453067"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predicted_qualities_loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7196.524270453067"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predicted_qualities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shaunak_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
